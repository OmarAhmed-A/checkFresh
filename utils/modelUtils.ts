import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-react-native';
import { Platform } from 'react-native';
import { imageProcessor } from './imageUtils';
import { bundleResourceIO, decodeJpeg } from '@tensorflow/tfjs-react-native';
import * as FileSystem from 'expo-file-system';

// Import the model files
const modelJSON = require('../assets/model/model.json');
const modelWeights = require('../assets/model/group1-shard1of1.bin');

// Initialize TensorFlow.js platform for React Native
if (Platform.OS !== 'web') {
  // Platform will be automatically registered when importing '@tensorflow/tfjs-react-native'
  console.log('TensorFlow.js React Native platform initialized');
}

// Comprehensive TensorFlow.js logging suppression
// Only set flags that are known to exist in this TensorFlow.js version
try {
  tf.env().set('DEBUG', false);
  // Skip WEBGL flags that might not be registered in React Native
  if (Platform.OS === 'web') {
    tf.env().set('WEBGL_DEBUG', false);
    tf.env().set('WEBGL_CPU_FORWARD', false);
    tf.env().set('WEBGL_PACK', false);
    tf.env().set('WEBGL_CONV_IM2COL', false);
    tf.env().set('WEBGL_MAX_TEXTURE_SIZE', 16384);
    tf.env().set('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION', 0);
    tf.env().set('WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE', false);
  }
} catch (error) {
  console.log('Some TensorFlow.js flags not available in this environment');
}

// Disable TensorFlow console output globally
(tf as any).enableProdMode?.();
if (typeof (tf as any).disableDeprecationWarnings === 'function') {
  (tf as any).disableDeprecationWarnings();
}

// Override console methods during TensorFlow operations
const silentConsole = {
  log: () => {},
  warn: () => {},
  info: () => {},
  debug: () => {},
  trace: () => {},
  error: (...args: any[]) => {
    // Only show actual errors, not TensorFlow warnings
    if (args.some(arg => typeof arg === 'string' && (
      arg.includes('Error') || 
      arg.includes('Failed') || 
      arg.includes('Cannot')
    ))) {
      console.error(...args);
    }
  }
};

export interface PredictionResult {
  className: string;
  confidence: number;
  isFresh: boolean;
  fruitType: string;
}

export class ModelService {
  private model: tf.GraphModel | tf.LayersModel | null = null;
  private isLoading = false;

  // Class labels matching the training data order
  private readonly CLASS_LABELS = [
    'freshapples',
    'freshbanana', 
    'freshoranges',
    'rottenapples',
    'rottenbanana',
    'rottenoranges'
  ];  
  
  async loadModel(): Promise<void> {
    if (this.model || this.isLoading) return;
    
    try {
      this.isLoading = true;
      console.log('Loading TensorFlow.js model...');
      
      // Initialize TensorFlow.js platform for React Native
      await tf.ready();
      console.log('TensorFlow.js platform initialized');
        if (Platform.OS === 'web') {        // For web, use direct asset path
        const modelUrl = '/assets/model/model.json';
        console.log('Loading model from web path:', modelUrl);
        
        this.model = await tf.loadGraphModel(modelUrl, { strict: false });
      } else {
        // For mobile platforms, use bundleResourceIO with single bin file
        try {
          console.log('Loading model using bundleResourceIO for mobile...');
          console.log('Model JSON loaded:', !!modelJSON);
          console.log('Model weights loaded:', !!modelWeights);
          
          // Check if modelJSON has the expected structure
          if (modelJSON && modelJSON.modelTopology) {
            console.log('Model topology found');
            console.log('Format:', modelJSON.format);
            console.log('Generated by:', modelJSON.generatedBy);
          } else {
            console.warn('Model JSON might be missing topology');
          }          // Load model directly without suppressing logs
          if (modelJSON.format === 'graph-model') {
            console.log('Loading graph model with bundleResourceIO...');
            this.model = await tf.loadGraphModel(
              bundleResourceIO(modelJSON, modelWeights)
            );
          } else {
            console.log('Loading layers model with bundleResourceIO...');
            this.model = await tf.loadLayersModel(
              bundleResourceIO(modelJSON, modelWeights),
              { strict: false }
            );
          }
          
          console.log('Model loaded successfully using bundleResourceIO');
        } catch (bundleError) {
          console.error('Bundle resource loading failed:', bundleError);
          
          // Fallback: try HTTP loading for development
          try {
            const modelUrl = 'http://localhost:8081/assets/model/model.json';
            console.log('Trying fallback HTTP loading:', modelUrl);
              this.model = await (async () => {
              // Use graph model loader for HTTP fallback since model format is graph-model
              if (modelJSON.format === 'graph-model') {
                return await tf.loadGraphModel(modelUrl);
              } else {
                return await tf.loadLayersModel(modelUrl, { strict: false });
              }
            })();
            
            console.log('Model loaded with HTTP fallback');
          } catch (httpError) {
            console.error('HTTP loading also failed:', httpError);
            throw httpError;
          }        }
      }
        if (this.model) {
        console.log('Model loaded successfully');
        console.log('Model input shape:', this.model.inputs[0].shape);
        
        // Handle output shape differently for GraphModel vs LayersModel
        if ('outputs' in this.model && this.model.outputs && this.model.outputs.length > 0) {
          console.log('Model output shape:', this.model.outputs[0].shape);
        } else if ('outputNames' in this.model) {
          console.log('Model output names:', this.model.outputNames);
        }
        
        // Only log layers for LayersModel
        if ('layers' in this.model) {
          console.log('Number of layers:', this.model.layers.length);
        }
      }
    } catch (error) {
      console.error('All model loading attempts failed:', error);
      
      // Final fallback: Create a mock model for testing
      console.log('Creating mock model for testing...');
      this.model = await this.createMockModel();
      console.log('Mock model created - app will work with random predictions');
    } finally {
      this.isLoading = false;
    }
  }
  
  async predict(imageUri: string): Promise<PredictionResult> {
    if (!this.model) {
      throw new Error('Model not loaded. Call loadModel() first.');
    }    try {
      console.log('Starting prediction...');
      console.log('TensorFlow memory info:', tf.memory());
      
      // Preprocess the image using our imageUtils
      const { tensor: imageTensor } = await imageProcessor.preprocessImage(imageUri);
      console.log('Image tensor shape:', imageTensor.shape);
      console.log('Image tensor dtype:', imageTensor.dtype);
        // Debug: Check tensor statistics
      // TEMPORARILY DISABLED - this might be causing stack overflow
      // const tensorData = await imageTensor.data();
      // const dataArray = Array.from(tensorData);
      // const minVal = Math.min(...dataArray);
      // const maxVal = Math.max(...dataArray);
      // const meanVal = dataArray.reduce((a: number, b: number) => a + b, 0) / dataArray.length;
      // console.log('Tensor stats - Min:', minVal.toFixed(1), 'Max:', maxVal.toFixed(1), 'Mean:', meanVal.toFixed(1), '(Expected: 0-255 range)');
      
      // Sample a few pixel values for debugging  
      // TEMPORARILY DISABLED
      // const samplePixels = dataArray.slice(0, 15).map(v => v.toFixed(1));
      // console.log('First 15 pixel values:', samplePixels);
      
      console.log('Skipping tensor data inspection to avoid stack overflow');
      
      console.log('About to start model prediction...');
      console.log('Model exists:', !!this.model);
      console.log('Model type check - has executeAsync:', 'executeAsync' in this.model);// Make prediction with proper method for model type
      console.log('Making prediction with model...');
      console.log('Model type:', this.model.constructor.name);
      
      let prediction: tf.Tensor;
        try {
        // For GraphModel, use executeAsync which is more stable
        if ('executeAsync' in this.model) {
          console.log('Attempting executeAsync...');
          const result = await (this.model as any).executeAsync(imageTensor);
          console.log('executeAsync completed');
          prediction = Array.isArray(result) ? result[0] : result;
        } else {
          console.log('Attempting predict method...');
          prediction = this.model.predict(imageTensor) as tf.Tensor;
          console.log('predict method completed');
        }
        
        console.log('Prediction obtained successfully');
        console.log('Prediction tensor shape:', prediction.shape);
        console.log('Prediction tensor dtype:', prediction.dtype);
          } catch (predictionError) {
        console.error('Model prediction method failed at step:', 
          'executeAsync' in this.model ? 'executeAsync' : 'predict');
        console.error('Prediction error:', predictionError);
        
        // Try alternative approach - create a smaller test tensor
        console.log('Attempting fallback prediction with test tensor...');
        try {
          const testTensor = tf.randomUniform([1, 240, 240, 3], 0, 255);
          
          if ('executeAsync' in this.model) {
            const testResult = await (this.model as any).executeAsync(testTensor);
            console.log('Test executeAsync succeeded - the issue is with the input tensor');
            testTensor.dispose();
            if (Array.isArray(testResult)) {
              testResult.forEach(t => t.dispose());
            } else {
              testResult.dispose();
            }
          } else {
            const testResult = this.model.predict(testTensor) as tf.Tensor;
            console.log('Test predict succeeded - the issue is with the input tensor');
            testTensor.dispose();
            testResult.dispose();
          }
        } catch (testError) {
          console.error('Even test tensor failed:', testError);
        }
        
        // Clean up input tensor before rethrowing
        imageTensor.dispose();
        throw predictionError;
      }
      const scores = await prediction.data();
      console.log('Raw prediction scores:', Array.from(scores));
      console.log('Scores length:', scores.length);
      
      // Find the class with highest probability
      const maxIndex = scores.indexOf(Math.max(...scores));
      const confidence = scores[maxIndex];
      const className = this.CLASS_LABELS[maxIndex];
      
      console.log('Predicted class index:', maxIndex);
      console.log('Predicted class name:', className);
      console.log('Confidence:', confidence);
      
      // Parse result
      const result = this.parseClassName(className, confidence);
      
      // Clean up tensors immediately to prevent memory issues
      imageTensor.dispose();
      prediction.dispose();
      
      // Force garbage collection if available
      if (typeof global !== 'undefined' && global.gc) {
        global.gc();
      }
      
      return result;} catch (error) {
      console.error('Prediction failed:', error);
      if (error instanceof Error) {
        console.error('Error details:', error.message);
        console.error('Error stack:', error.stack);
      }
      
      // Return a mock prediction for testing
      return {
        className: 'freshapples',
        confidence: 0.60,
        isFresh: true,
        fruitType: 'apple'
      };
    }
  }  
  
  private async createMockModel(): Promise<tf.LayersModel> {
    // Create a simple mock model for testing when the real model fails to load
    const model = tf.sequential({
      layers: [
        tf.layers.flatten({ inputShape: [240, 240, 3] }),
        tf.layers.dense({ units: 128, activation: 'relu' }),
        tf.layers.dense({ units: 6, activation: 'softmax' }) // 6 classes
      ]
    });
    
    console.log('Mock model created with input shape: [null, 240, 240, 3]');
    return model;
  }

  private parseClassName(className: string, confidence: number): PredictionResult {
    const isFresh = className.startsWith('fresh');
    const fruitType = className.replace('fresh', '').replace('rotten', '');
    
    return {
      className,
      confidence: Math.round(confidence * 100) / 100,
      isFresh,
      fruitType
    };
  }

  isModelLoaded(): boolean {
    return this.model !== null;
  }

  /**
   * Suppress TensorFlow.js verbose output during model operations
   */
  private async suppressTensorFlowLogs<T>(operation: () => Promise<T>): Promise<T> {
    // Store original console methods
    const originalConsole = {
      log: console.log,
      warn: console.warn,
      info: console.info,
      debug: console.debug,
      trace: console.trace
    };

    try {
      // Replace console methods with silent versions
      Object.assign(console, silentConsole);
      
      // Execute the operation
      const result = await operation();
      
      return result;
    } finally {
      // Always restore original console methods
      Object.assign(console, originalConsole);
    }
  }

  /**
   * Patch model JSON to fix compatibility issues with different TensorFlow.js versions
   */
  private patchModelJSON(originalModelJSON: any): any {
    // Create a patched version of the model JSON to handle validation issues
    const patched = JSON.parse(JSON.stringify(originalModelJSON));
    
    // Add missing className and config properties if they don't exist
    if (patched.modelTopology && patched.modelTopology.model_config) {
      const config = patched.modelTopology.model_config;
      
      // Ensure className exists
      if (!config.className) {
        config.className = 'Sequential';
      }
      
      // Ensure config property exists
      if (!config.config) {
        config.config = {};
      }
      
      // Handle layers validation
      if (config.layers) {
        config.layers.forEach((layer: any, index: number) => {
          if (!layer.className) {
            // Try to infer className from class_name or provide default
            layer.className = layer.class_name || `Layer${index}`;
          }
          if (!layer.config) {
            layer.config = layer.config || {};
          }
        });
      }
    }
    
    return patched;
  }
}

// Singleton instance
export const modelService = new ModelService();
